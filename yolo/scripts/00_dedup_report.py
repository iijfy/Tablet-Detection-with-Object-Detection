# -*- coding: utf-8 -*-
"""
ë°ì´í„° ì¤‘ë³µ/ëˆ„ìˆ˜ ì§„ë‹¨ ë¦¬í¬íŠ¸ ìƒì„±ê¸° ğŸ˜Š
- ì™„ì „ ì¤‘ë³µ: MD5 í•´ì‹œë¡œ ë™ì¼ ì´ë¯¸ì§€ ê·¸ë£¹ ì°¾ê¸°
- ê·¼ì‚¬ ì¤‘ë³µ: aHash(8x8 í‰ê·  í•´ì‹œ) í•´ë°ê±°ë¦¬ë¡œ ìœ ì‚¬ í›„ë³´ ì°¾ê¸°
- ë¼ë²¨ ì¤‘ë³µ: ë™ì¼ ì´ë¯¸ì§€ ë‚´ IoU >= 0.95 ë°•ìŠ¤ ìŒ(ê°™ì€ category)
- ìŠ¤í”Œë¦¿ ëˆ„ìˆ˜: train/val/test ì‚¬ì´ì— ë™ì¼ ì´ë¯¸ì§€(í•´ì‹œ) ì¡´ì¬
ì¶œë ¥: CSV ì—¬ëŸ¬ ê°œ + ìš”ì•½ ì¶œë ¥
"""

import os, sys, csv, json, glob, hashlib
from collections import defaultdict
from PIL import Image

# ========= ì‚¬ìš©ì í™˜ê²½ ê²½ë¡œ ì„¤ì • (ë„¤ í™˜ê²½ ë°˜ì˜) =========
ANN_JSON_DIR = "/mnt/nas/jayden_code/ai05-level1-project/train_annotations"  # COCO jsonë“¤(í•˜ìœ„ í´ë” ì¬ê·€)
IMG_TRAIN_DIR = "/mnt/nas/jayden_code/ai05-level1-project/train_images"      # ì›ë³¸ train ì´ë¯¸ì§€ ë£¨íŠ¸
IMG_TEST_DIR  = "/mnt/nas/jayden_code/ai05-level1-project/test_images"       # ì›ë³¸ test ì´ë¯¸ì§€ ë£¨íŠ¸

# (ì„ íƒ) YOLOë¡œ ë³€í™˜ëœ ìµœì¢… split ê²½ë¡œ(ìˆë‹¤ë©´ ëˆ„ìˆ˜ ì ê²€ì— í™œìš©)
YOLO_TRAIN_IMG = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data_clean/train/images")
YOLO_VAL_IMG   = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data_clean/val/images")

CLASSMAP_CSV   = os.path.join(os.path.dirname(os.path.dirname(__file__)), "metadata/class_map.csv")
OUT_DIR        = os.path.join(os.path.dirname(os.path.dirname(__file__)), "reports")  # ë¦¬í¬íŠ¸ ì €ì¥ í´ë”

# ========= ìœ í‹¸ =========
def info(*a): print("[INFO]", *a)
def warn(*a): print("[WARN]", *a)
os.makedirs(OUT_DIR, exist_ok=True)

def md5_of_file(path, chunk=8192):
    """íŒŒì¼ì˜ MD5 í•´ì‹œ ê³„ì‚°í•´ìš” ğŸ”"""
    h = hashlib.md5()
    with open(path, "rb") as f:
        while True:
            b = f.read(chunk)
            if not b: break
            h.update(b)
    return h.hexdigest()

def average_hash(img_path, size=8):
    """aHash ë§Œë“¤ê¸° ğŸ§©: ì‘ê²Œ(8x8) ì¤„ì´ê³  í‰ê· ë³´ë‹¤ í° í”½ì…€=1, ì‘ìœ¼ë©´=0 ë¹„íŠ¸"""
    try:
        with Image.open(img_path) as im:
            im = im.convert("L").resize((size, size))  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ + 8x8
            pixels = list(im.getdata())
    except Exception:
        return None  # ì—´ê¸° ì‹¤íŒ¨
    avg = sum(pixels) / len(pixels)
    bits = [1 if p >= avg else 0 for p in pixels]
    # 64ë¹„íŠ¸ë¥¼ 16ì§„ ë¬¸ìì—´ë¡œ
    val = 0
    for b in bits:
        val = (val << 1) | b
    return f"{val:016x}"

def hamming_hex(h1, h2):
    """16ì§„ í•´ì‹œ ë¬¸ìì—´ ì‚¬ì´ í•´ë° ê±°ë¦¬(ë¹„íŠ¸ ë‹¨ìœ„) ğŸ“"""
    if (h1 is None) or (h2 is None):
        return 999
    return bin(int(h1, 16) ^ int(h2, 16)).count("1")

def load_coco_annotations(json_root):
    """
    COCO jsonë“¤ì„ ëª¨ë‘ ì½ì–´ ì´ë¯¸ì§€ë³„ ë¼ë²¨ ìˆ˜ì§‘í•´ìš” ğŸ“š
    ë°˜í™˜:
      img_boxes[fname] = [(x,y,w,h,cid), ...]
      img_size[fname]  = (W,H)
      class_ids        = set(cid)
    """
    img_boxes = defaultdict(list)
    img_size  = {}
    class_ids = set()

    json_files = glob.glob(os.path.join(json_root, "**/*.json"), recursive=True)
    assert json_files, f"JSONì´ ì—†ìŠµë‹ˆë‹¤: {json_root}"

    for jp in json_files:
        try:
            d = json.load(open(jp, "r", encoding="utf-8"))
        except Exception as e:
            warn(f"JSON ë¡œë“œ ì‹¤íŒ¨: {jp} -> {e}")
            continue

        imap = {}
        for im in d.get("images", []):
            try:
                _id = int(im["id"])
                fname = im["file_name"]
                W = int(im["width"]); H=int(im["height"])
                imap[_id] = (fname, W, H)
            except Exception:
                continue

        for a in d.get("annotations", []):
            if "bbox" not in a or "image_id" not in a or "category_id" not in a:
                continue
            x,y,w,h = a["bbox"]
            img_id = int(a["image_id"])
            if img_id not in imap: 
                continue
            fname,W,H = imap[img_id]
            # í´ë¦¬í•‘ (ìŒìˆ˜/ê²½ê³„ ì´ˆê³¼ ë°©ì§€)
            x1 = max(0.0, float(x)); y1 = max(0.0, float(y))
            x2 = min(float(W), x1 + float(w))
            y2 = min(float(H), y1 + float(h))
            cw = max(0.0, x2 - x1); ch = max(0.0, y2 - y1)
            if cw <= 0 or ch <= 0: 
                continue

            cid = int(a["category_id"])
            img_boxes[fname].append((x1,y1,cw,ch,cid))
            img_size[fname] = (W,H)
            class_ids.add(cid)

    return img_boxes, img_size, class_ids

def iou_xywh(b1, b2):
    """IoU ê³„ì‚°ê¸° ğŸ“: (x,y,w,h)"""
    x1,y1,w1,h1 = b1
    x2,y2,w2,h2 = b2
    xa1, ya1 = x1, y1
    xa2, ya2 = x1+w1, y1+h1
    xb1, yb1 = x2, y2
    xb2, yb2 = x2+w2, y2+h2
    inter_w = max(0, min(xa2, xb2) - max(xa1, xb1))
    inter_h = max(0, min(ya2, yb2) - max(ya1, yb1))
    inter = inter_w * inter_h
    area1 = w1*h1; area2 = w2*h2
    union = area1 + area2 - inter
    if union <= 0: return 0.0
    return inter / union

# ========= 1) ì™„ì „ì¤‘ë³µ/ê·¼ì‚¬ì¤‘ë³µ íƒì§€ =========
def scan_images_md5_ahash(root_dir):
    """ë£¨íŠ¸ ì•„ë˜ ëª¨ë“  ì´ë¯¸ì§€ì˜ MD5/aHash ìŠ¤ìº” ğŸ”"""
    exts = {".jpg",".jpeg",".png",".bmp",".webp",".tif",".tiff"}
    paths = []
    for r,_,fs in os.walk(root_dir):
        for f in fs:
            if os.path.splitext(f.lower())[1] in exts:
                paths.append(os.path.join(r,f))

    md5_map = defaultdict(list)
    ahash_map = {}
    for p in paths:
        try:
            m = md5_of_file(p)
        except Exception as e:
            warn(f"MD5 ì‹¤íŒ¨: {p} -> {e}")
            continue
        md5_map[m].append(p)
        ahash_map[p] = average_hash(p)
    return md5_map, ahash_map

# ========= 2) ìŠ¤í”Œë¦¿ ëˆ„ìˆ˜ ì ê²€ =========
def leak_between_splits(split_dirs):
    """
    split_dirs: [('train', dir1), ('val', dir2), ('test', dir3)]
    ë™ì¼ MD5 í•´ì‹œê°€ ì—¬ëŸ¬ splitì— ì¡´ì¬í•˜ë©´ ëˆ„ìˆ˜ í›„ë³´ âš ï¸
    """
    tag_of = {}  # md5 -> set(tags)
    file_of = defaultdict(list)  # md5 -> [paths]
    for tag, d in split_dirs:
        if not d or not os.path.exists(d): 
            continue
        md5_map, _ = scan_images_md5_ahash(d)
        for h, paths in md5_map.items():
            tag_of.setdefault(h, set()).add(tag)
            file_of[h].extend(paths)
    leaks = []
    for h, tags in tag_of.items():
        if len(tags) >= 2:
            leaks.append((h, ",".join(sorted(tags)), len(file_of[h]), file_of[h][:10]))
    return leaks

# ========= 3) ë¼ë²¨ ì¤‘ë³µ ì ê²€ =========
def label_duplicates(img_boxes, thr=0.95):
    """
    ê°™ì€ ì´ë¯¸ì§€ ë‚´ì—ì„œ ê°™ì€ category ê°„ IoU>=thr ì¸ ë°•ìŠ¤ ìŒì„ ì°¾ëŠ”ë‹¤ ğŸ“
    """
    rows = []
    for fname, boxes in img_boxes.items():
        n = len(boxes)
        for i in range(n):
            x1,y1,w1,h1,c1 = boxes[i]
            for j in range(i+1, n):
                x2,y2,w2,h2,c2 = boxes[j]
                if c1 != c2: 
                    continue
                iou = iou_xywh((x1,y1,w1,h1), (x2,y2,w2,h2))
                if iou >= thr:
                    rows.append([fname, i, j, c1, round(iou,4)])
    return rows

# ========= 4) í´ë˜ìŠ¤ ë¶„í¬ í‘œ =========
def class_distribution(img_boxes):
    cnt = defaultdict(int)
    for fname, boxes in img_boxes.items():
        for _,_,_,_,cid in boxes:
            cnt[cid]+=1
    rows = sorted(cnt.items(), key=lambda x: x[0])
    return rows

# ========= ë©”ì¸ =========
def main():
    info("COCO ì£¼ì„ ë¡œë“œ ì¤‘...")
    img_boxes, img_size, class_ids = load_coco_annotations(ANN_JSON_DIR)
    info(f"ë¼ë²¨ ì´ë¯¸ì§€ ìˆ˜: {len(img_boxes)} | í´ë˜ìŠ¤ ìˆ˜: {len(class_ids)}")

    # (A) ì›ë³¸ train/test ì´ë¯¸ì§€ ì¤‘ë³µ
    info("ì›ë³¸ train_images ìŠ¤ìº”(ì™„ì „/ê·¼ì‚¬ ì¤‘ë³µ)...")
    md5_train, ahash_train = scan_images_md5_ahash(IMG_TRAIN_DIR)
    info("ì›ë³¸ test_images ìŠ¤ìº”(ì™„ì „/ê·¼ì‚¬ ì¤‘ë³µ)...")
    md5_test, ahash_test   = scan_images_md5_ahash(IMG_TEST_DIR)

    # ì™„ì „ ì¤‘ë³µ ê·¸ë£¹(ì›ë³¸ train ë‚´)
    dup_exact_rows = []
    for h, paths in md5_train.items():
        if len(paths) >= 2:
            dup_exact_rows.append([h, len(paths)] + paths[:10])
    with open(os.path.join(OUT_DIR, "duplicate_exact_train.csv"), "w", newline="", encoding="utf-8") as f:
        cw = csv.writer(f); cw.writerow(["md5","count","sample_paths_up_to_10"])
        for r in dup_exact_rows: cw.writerow(r)

    # ê·¼ì‚¬ ì¤‘ë³µ í›„ë³´(ì›ë³¸ train ë‚´) â€” í•´ë°ê±°ë¦¬ <= 5
    # ê°„ë‹¨íˆ ìƒìœ„ Nê°œë§Œ í˜ì–´ë§(ì „ì²´ í˜ì–´ O(N^2) ë°©ì§€í•˜ë ¤ë©´ í•´ì‹œë²„í‚·/ê·¸ë¦¬ë“œê°€ ì¢‹ìŒ)
    paths = list(ahash_train.keys())
    approx_rows = []
    N = len(paths)
    LIM = 5000  # ë„ˆë¬´ í¬ë©´ ë¶€ë¶„ ìƒ˜í”Œë§Œ ë¹„êµ
    step_paths = paths[:min(N, LIM)]
    for i in range(len(step_paths)):
        for j in range(i+1, len(step_paths)):
            d = hamming_hex(ahash_train[step_paths[i]], ahash_train[step_paths[j]])
            if d <= 5:
                approx_rows.append([d, step_paths[i], step_paths[j]])
    with open(os.path.join(OUT_DIR, "duplicate_ahash_candidates_train.csv"), "w", newline="", encoding="utf-8") as f:
        cw = csv.writer(f); cw.writerow(["hamming_distance","path_a","path_b"])
        cw.writerows(sorted(approx_rows, key=lambda r:r[0]))

    # (B) ìŠ¤í”Œë¦¿ ëˆ„ìˆ˜ (YOLO ë³€í™˜ split ê¸°ì¤€ + ì›ë³¸ train/test ê¸°ì¤€ ëª¨ë‘ ì ê²€)
    leaks = leak_between_splits([
        ("yolo_train", YOLO_TRAIN_IMG if os.path.exists(YOLO_TRAIN_IMG) else None),
        ("yolo_val",   YOLO_VAL_IMG   if os.path.exists(YOLO_VAL_IMG)   else None),
        ("orig_train", IMG_TRAIN_DIR),
        ("orig_test",  IMG_TEST_DIR),
    ])
    with open(os.path.join(OUT_DIR, "split_leakage.csv"), "w", newline="", encoding="utf-8") as f:
        cw = csv.writer(f); cw.writerow(["md5","splits","count","sample_paths_up_to_10"])
        for h, tags, c, sample in leaks:
            cw.writerow([h, tags, c, " | ".join(sample)])

    # (C) ë¼ë²¨ ì¤‘ë³µ (ë™ì¼ ì´ë¯¸ì§€ ë‚´ë¶€ IoU>=0.95 ìŒ)
    rows = label_duplicates(img_boxes, thr=0.95)
    with open(os.path.join(OUT_DIR, "label_duplicates_iou095.csv"), "w", newline="", encoding="utf-8") as f:
        cw = csv.writer(f); cw.writerow(["image_file","box_idx_a","box_idx_b","category_id","iou"])
        cw.writerows(rows)

    # (D) í´ë˜ìŠ¤ ë¶„í¬ í‘œ
    dist = class_distribution(img_boxes)
    with open(os.path.join(OUT_DIR, "class_distribution_overall.csv"), "w", newline="", encoding="utf-8") as f:
        cw = csv.writer(f); cw.writerow(["category_id","count"])
        cw.writerows(dist)

    # (E) ìš”ì•½ í”„ë¦°íŠ¸
    n_exact_groups = sum(1 for _,paths in md5_train.items() if len(paths)>=2)
    info("===== ìš”ì•½ =====")
    info(f"ì™„ì „ ì¤‘ë³µ ê·¸ë£¹ ìˆ˜(train): {n_exact_groups}")
    info(f"ê·¼ì‚¬ ì¤‘ë³µ í›„ë³´(train, aHash<=5, max {len(step_paths)}ê°œ ë¹„êµ): {len(approx_rows)}")
    info(f"ìŠ¤í”Œë¦¿ ëˆ„ìˆ˜ í›„ë³´ ê°œìˆ˜: {len(leaks)}")
    info(f"ë¼ë²¨ ì¤‘ë³µ ìŒ(IoU>=0.95): {len(rows)}")
    info(f"í´ë˜ìŠ¤ ë¶„í¬ íŒŒì¼: {os.path.join(OUT_DIR,'class_distribution_overall.csv')}")
    info(f"ë¦¬í¬íŠ¸ í´ë”: {OUT_DIR}")

if __name__ == "__main__":
    main()